{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    },
    "colab": {
      "name": "Copy of AMMI_EXPRL_Exercise.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jean-KOUAGOU/AIMS-MCFDM2019/blob/master/Copy_of_AMMI_EXPRL_Exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UWM-gGi76pF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/rlgammazero/mvarl_hands_on.git > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5EbzJ1A78Bk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, './mvarl_hands_on/exploration')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDPYhYvR7yKZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from riverswim import RiverSwim\n",
        "import cvxpy as cp\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5twn7Qv7yKf",
        "colab_type": "text"
      },
      "source": [
        "# Finite-Horizon MDPs\n",
        "We consider finite horizon problems with horizon $H$. For simplicity, we consider MDPs with stationary transitions and rewards, ie these functions do not depend on the stage ($p_h =p$, $r_h=r$ for any $h \\in [H]$).\n",
        "\n",
        "The value of a policy or the optimal value function can be computed using *backward induction*.\n",
        "\n",
        "\n",
        "Given a deterministic (non-stationary) policy $\\pi = (\\pi_1, \\pi_2, \\ldots, \\pi_H)$, backward induction applies the Bellman operator defined as\n",
        "$$\n",
        "V_h^\\pi(s) = \\sum_{s'} p(s'|s,\\pi_h(s)) \\left( r(s,\\pi_h(s),s') + V_{h+1}^\\pi(s')\\right)\n",
        "$$\n",
        "where $V_{H+1}(s) = 0$, for any $s$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7l8433mz7yKh",
        "colab_type": "code",
        "outputId": "6ffae5f6-e7b4-4f1c-a1d1-56316577fdf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "env = RiverSwim(6)\n",
        "H = 10\n",
        "print(\"Reward matrix: \", env.R.shape)\n",
        "print(env.R)\n",
        "print()\n",
        "print(\"Transition matrix: \", env.P.shape)\n",
        "print(\"Transitions probabilities for state s_1:\")\n",
        "print(env.P[1])\n",
        "\n",
        "print(env.P[0, 1, 1])"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reward matrix:  (6, 2)\n",
            "[[0.005 0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    1.   ]]\n",
            "\n",
            "Transition matrix:  (6, 2, 6)\n",
            "Transitions probabilities for state s_1:\n",
            "[[1.   0.   0.   0.   0.   0.  ]\n",
            " [0.05 0.6  0.35 0.   0.   0.  ]]\n",
            "0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiXHdYle7yKm",
        "colab_type": "text"
      },
      "source": [
        "# Backward induction (aka Value Iteration)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_KZXn8A7yKo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backward_induction(P, R, H):\n",
        "    \"\"\"\n",
        "        Parameters:\n",
        "            P: transition function (S,A,S)-dim matrix\n",
        "            R: reward function (S,A)-dim matrix\n",
        "            H: horizon\n",
        "\n",
        "        Returns:\n",
        "            The optimal V-function\n",
        "            The optimal policy\n",
        "    \"\"\"\n",
        "    S, A = P.shape[0], P.shape[1]\n",
        "    policy = np.zeros((H, S), dtype=np.int)\n",
        "    V = np.zeros((H + 1, S))\n",
        "    for h in reversed(range(H)):\n",
        "        for s in range(S):\n",
        "            \"\"\" \n",
        "            Here, we compute V^*(h, s) using the Bellman optimality equation:\n",
        "\n",
        "            V[h, s] = max_a  R[s, a] + sum_{s'} P[s, a, s']*V[h+1, s']\n",
        "            \"\"\"\n",
        "            for a in range(A):\n",
        "                tmp = np.dot(P[s, a], R[s, a] + V[h + 1])\n",
        "                if (a == 0) or (tmp > V[h, s]):\n",
        "                    policy[h, s] = a\n",
        "                    V[h, s] = tmp\n",
        "    return V, policy\n",
        "\n",
        "def policy_evaluation(P, R, H, policy):\n",
        "    \"\"\"\n",
        "        Parameters:\n",
        "            P: transition function (S,A,S)-dim matrix\n",
        "            R: reward function (S,A)-dim matrix\n",
        "            H: horizon\n",
        "            policy: policy (H,S)-dim matrix\n",
        "\n",
        "        Returns:\n",
        "            The V-function of the given policy\n",
        "    \"\"\"\n",
        "    S, A = P.shape[0], P.shape[1]\n",
        "    V = np.zeros((H + 1, S))\n",
        "    for h in reversed(range(H)):\n",
        "        for s in range(S):\n",
        "            \"\"\" \n",
        "            Here, we compute V^pi(h, s) using the Bellman equation for the policy pi:\n",
        "\n",
        "            a = policy[h,s]\n",
        "            V[h, s] =  R[s, a] + sum_{s'} P[s, a, s']*V[h+1, s']\n",
        "            \"\"\"\n",
        "            a = policy[h,s]\n",
        "            # complete the policy evalution here\n",
        "            V[h, s] = R[s, a] + P[s,a,:].dot(V[h+1,:])\n",
        "    return V"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvmhRCN07yKs",
        "colab_type": "text"
      },
      "source": [
        "Compute solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVjkydge7yKt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "984d835f-0453-437f-e18c-e09ee2358bb1"
      },
      "source": [
        "Vstar, POLstar = backward_induction(env.P, env.R, H)\n",
        "\n",
        "print(\"Optimal policy\")\n",
        "print(np.round(Vstar))\n",
        "\n",
        "print(POLstar)\n",
        "\n",
        "#to test the implementation\n",
        "V_policy = policy_evaluation(env.P, env.R, H, POLstar)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimal policy\n",
            "[[0. 1. 1. 2. 4. 5.]\n",
            " [0. 0. 1. 2. 3. 5.]\n",
            " [0. 0. 1. 2. 3. 4.]\n",
            " [0. 0. 1. 1. 3. 4.]\n",
            " [0. 0. 0. 1. 2. 3.]\n",
            " [0. 0. 0. 1. 2. 3.]\n",
            " [0. 0. 0. 0. 1. 3.]\n",
            " [0. 0. 0. 0. 1. 2.]\n",
            " [0. 0. 0. 0. 0. 2.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n",
            "[[1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1]\n",
            " [0 1 1 1 1 1]\n",
            " [0 1 1 1 1 1]\n",
            " [0 0 1 1 1 1]\n",
            " [0 0 0 1 1 1]\n",
            " [0 0 0 0 1 1]\n",
            " [0 0 0 0 0 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJ7sdku_3pmq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "938cd01b-33c8-4890-877e-0870a296d8c5"
      },
      "source": [
        "print(np.abs(V_policy-Vstar).sum())"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7.16093850883226e-15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEYM9-e97w2L",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "d387d1af-9d4b-40d5-86f9-56b8472970a7"
      },
      "source": [
        "S, A = env.R.shape # number of states and actions\n",
        "Phat = np.zeros((S,A,S))\n",
        "Rhat = np.zeros((S,A))\n",
        "\n",
        "N_sa = np.zeros((S,A)) # number of visits to each state-action\n",
        "N_sas = np.zeros((S,A,S)) \n",
        "S_sa = np.zeros((S,A)) # sum of rewards obtained when visiting each state-action\n",
        "\n",
        "# Interact with the system\n",
        "nb_episodes = 200\n",
        "# Loop over episodes\n",
        "for ep in range(nb_episodes):\n",
        "  state = env.reset()\n",
        "  for h in range(H):\n",
        "    action = np.random.choice(A)\n",
        "    next_state, reward, done, _ = env.step(action)\n",
        "    # Update estimates\n",
        "    N_sa[state, action] += 1\n",
        "    N_sas[state, action, next_state] += 1\n",
        "    S_sa[state, action] += reward\n",
        "\n",
        "    Rhat[state, action] = S_sa[state, action]/N_sa[state, action]\n",
        "    Phat[state, action, :] = N_sas[state, action, :]/N_sa[state, action]\n",
        "    state = next_state\n",
        "\n",
        "print(Rhat)\n",
        "print(env.R)\n",
        "\n",
        "\n",
        "beta_r = np.zeros((S,A))\n",
        "for s in range(S):\n",
        "  for a in range(A):\n",
        "    n = max(N_sa[s,a], 1)\n",
        "    beta_r[s,a] = np.sqrt(np.log(S*A*n)/n)\n",
        "\n",
        "#print(beta_r)\n",
        "\n",
        "\n",
        "for s in range(S):\n",
        "  for a in range(A):\n",
        "    norm_1 = np.abs(env.P[s,a,:]-Phat[s,a,:]).sum()\n",
        "    print(norm_1, beta_r[s,a])\n",
        "# print(Phat)\n",
        "# print(env.P)\n"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.005 0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    1.   ]]\n",
            "[[0.005 0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    1.   ]]\n",
            "0.0 0.12148830723918205\n",
            "0.0073959938366717215 0.11750054497130999\n",
            "0.0 0.16976009063195388\n",
            "0.2303030303030303 0.17473850325183224\n",
            "0.0 0.2884054052930078\n",
            "0.06582278481012666 0.2945572627541253\n",
            "0.0 0.559312602998707\n",
            "0.19999999999999996 0.6049772120503654\n",
            "0.0 1.0929347248663588\n",
            "0.10000000000000005 0.9049137596723901\n",
            "0.0 1.5763586678760644\n",
            "1.2 1.5763586678760644\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1nCGWuw7yKy",
        "colab_type": "text"
      },
      "source": [
        "## UCRL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoWtsh7v7yK0",
        "colab_type": "text"
      },
      "source": [
        "UCRL is an algorithm for efficient exploration in finite-horizon tabular MDP.\n",
        "In this setting, the regret is defined as\n",
        "$$R(K) = \\sum_{k=1}^K V^\\star_1(s_{k,1}) - V^{\\pi_k}_1(s_{k,1})$$\n",
        "UCBVI enjoys a regret bound of order $O(\\sqrt{HSAK})$.\n",
        "\n",
        "The structure of the algorithm is as follow\n",
        "\n",
        "For $k = 1, \\ldots, K$ do<br>\n",
        "> Solve optimistic planning problem -> $(V_k, Q_k, \\pi_k)$<br>\n",
        "> Execute the optimistic policy $\\pi_k$ for $H$ steps<br>\n",
        ">> for $h=1, \\ldots, H$<br>\n",
        ">>> $a_{k,h} = \\pi(s_{k,h})$<br>\n",
        ">>> execute $a_{k,h}$, observe $r_{k,h}$ and $s_{k, h+1}$<br>\n",
        ">>> $N(s_{k,h}, a_{k,h}, s_{k,h+1}) += 1$ (update also estimated reward and transitions)\n",
        "\n",
        "<font color='#ed7d31'>Optimistic planning</font>\n",
        "At each episode, UCRL computes the optimal solution by solving the following \"extended\" problem\n",
        "$$\n",
        "V_h^\\star(s) =  \\max_{r \\in B_r(s,a)} r + \\max_{p \\in B_p(s,a)} \\sum_{s'} p(s') V_{h+1}(s') \n",
        "$$\n",
        "where $V_{H+1}(s) = 0$ and $B_r(s,a)$ and $B_p(s,a)$ are confidence intervals on the estimated transitions and rewards:\n",
        "\n",
        "$$\n",
        "B_r(s,a) = \\{ r(s,a):  |r_s, a) - \\hat{r}(s,a)| \\leq \\beta_r(s,a)  \\}\n",
        "$$\n",
        "\n",
        "$$\n",
        "B_p(s,a) = \\{ p(\\cdot|s,a):  ||p(\\cdot|s,a) - \\hat{p}(\\cdot|s,a)||_1 \\leq \\beta_p(s,a)  \\}\n",
        "$$\n",
        "where \n",
        "\n",
        "$$\n",
        "\\beta_r(s, a) = \\sqrt{ \\frac{ \\log(S A N^+(s,a) / \\delta)}{ N^+(s, a)}  } \\\\\n",
        "\\beta_p(s, a) = \\sqrt{ \\frac{  S \\log(S A N^+(s,a) / \\delta)}{ N^+(s, a)}  }\n",
        "$$\n",
        "and where  $N^+(s, a) = \\max(1, N(s, a))$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ji7IShLN7yK1",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "The following function computes:\n",
        "$$\\max_{x \\in B_p} \\sum_{s'} x(s') V(s') $$\n",
        "where $B_p = [P-\\beta, P+\\beta]$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57zVp32L7yK3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def LPprobaH(v, P, beta, verbose=0):\n",
        "    \"\"\"\n",
        "        max_x v^T x\n",
        "        s.t.    0 <= x_i <= 1\n",
        "                \\sum_i |x_i - p_i| \\leq beta\n",
        "                \\sum_i x_i = 1\n",
        "    \"\"\"\n",
        "    sorted_idxs = np.argsort(v)[::-1]\n",
        "\n",
        "    pest = P.copy()\n",
        "    idx = sorted_idxs[0]\n",
        "    pest[idx] = min(1., P[idx] + beta / 2.)\n",
        "    delta = pest.sum()\n",
        "    j = len(P)-1\n",
        "    while delta > 1:\n",
        "        idx_j = sorted_idxs[j]\n",
        "        m = max(0, 1. - delta + pest[idx_j])\n",
        "        delta = delta - pest[idx_j] + m\n",
        "        pest[idx_j] = m\n",
        "        j -= 1\n",
        "    w = np.dot(pest, v)\n",
        "    return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pk0v7jg67yK7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def UCRL(mdp, H, nb_episodes, VSTAR=0):\n",
        "    S, A = mdp.Ns, mdp.Na\n",
        "    policy = np.zeros((H, S), dtype=np.int)\n",
        "    Phat = np.ones((S,A,S)) / S\n",
        "    Rhat = np.zeros((S,A))\n",
        "    N_sas = np.zeros((S,A,S), dtype=np.int)\n",
        "    N_sa = np.zeros((S,A), dtype=np.int)\n",
        "    regret = np.zeros((nb_episodes,))\n",
        "    V = np.zeros((H + 1, S))\n",
        "    \n",
        "    delta = 0.1\n",
        "    \n",
        "    for k in range(nb_episodes):\n",
        "        \n",
        "        # compute optimistic solution\n",
        "        # 1. compute confidence intervals\n",
        "        N = np.maximum(N_sa, 1)\n",
        "        LOGT = np.log(S * A * N / delta)\n",
        "        beta_r = np.sqrt(LOGT/N)\n",
        "        beta_p = np.sqrt(S*LOGT/N)\n",
        "        \n",
        "        # 2. run extended value iteration\n",
        "        V.fill(0)\n",
        "        for h in reversed(range(H)):\n",
        "            for s in range(S):\n",
        "              tmp=np.zeros(A)\n",
        "              for a in range(A):\n",
        "                  dotp = LPprobaH(V[h + 1], Phat[s, a], beta_p[s, a])\n",
        "                  tmp[a] = dotp+Rhat[s,a]+beta_r[s,a]\n",
        "              V[h, s] = tmp.max()\n",
        "              policy[h, s] = tmp.argmax()\n",
        "        \n",
        "        # execute policy\n",
        "        initial_state = state = mdp.reset()\n",
        "        for h in range(H):\n",
        "            action = policy[h][state]\n",
        "            next_state, reward, done, _ = mdp.step(action)\n",
        "            \n",
        "            # update estimates (Phat, Rhat, N_sa, N_sas)\n",
        "            N_sa[state, action] += 1\n",
        "            N_sas[state, action, next_state] += 1\n",
        "            S_sa[state, action] += reward\n",
        "\n",
        "            Rhat[state, action] = S_sa[state, action]/N_sa[state, action]\n",
        "            Phat[state, action, :] = N_sas[state, action, :]/N_sa[state, action]\n",
        "            state = next_state\n",
        "        \n",
        "        # update regret\n",
        "        Vpi = policy_evaluation(mdp.P, mdp.R, H, policy)\n",
        "        regret[k] = VSTAR[0][initial_state] - Vpi[0][initial_state]\n",
        "        \n",
        "        if k % 50 == 0:\n",
        "            print(\"regret[{}]: {}\".format(k, regret[k]))\n",
        "    return regret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-UTr3Gq7yK_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "dc5cf254-6852-4fec-af72-41a3941e8b41"
      },
      "source": [
        "nb_repetitions = 5\n",
        "nb_episodes = 750\n",
        "regrets = np.zeros((nb_repetitions, nb_episodes))\n",
        "for it in range(nb_repetitions):\n",
        "    print(\"Running simulation: {}\".format(it))\n",
        "    regrets[it] = UCRL(mdp=env, H=H, nb_episodes=nb_episodes, VSTAR=Vstar)"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running simulation: 0\n",
            "regret[0]: 0.3023839779999999\n",
            "regret[50]: 0.3523839779999999\n",
            "regret[100]: 0.3381416968749999\n",
            "regret[150]: 0.3370587416249999\n",
            "regret[200]: 0.038639677569999975\n",
            "regret[250]: 0.038639677569999975\n",
            "regret[300]: 0.037267573081250005\n",
            "regret[350]: 0.03513761854999997\n",
            "regret[400]: 0.03495190939999998\n",
            "regret[450]: 0.03363150300000001\n",
            "regret[500]: 0.031235826999999994\n",
            "regret[550]: 0.031235826999999994\n",
            "regret[600]: 0.030874186999999997\n",
            "regret[650]: 0.039653253312500025\n",
            "regret[700]: 0.030874186999999997\n",
            "Running simulation: 1\n",
            "regret[0]: 0.3023839779999999\n",
            "regret[50]: 0.3523839779999999\n",
            "regret[100]: 0.3392420322999999\n",
            "regret[150]: 0.038639677569999975\n",
            "regret[200]: 0.038498979553750035\n",
            "regret[250]: 0.10931475953875\n",
            "regret[300]: 0.03495190939999998\n",
            "regret[350]: 0.03495190939999998\n",
            "regret[400]: 0.03363150300000001\n",
            "regret[450]: 0.03363150300000001\n",
            "regret[500]: 0.031235826999999994\n",
            "regret[550]: 0.031235826999999994\n",
            "regret[600]: 0.039653253312500025\n",
            "regret[650]: 0.039653253312500025\n",
            "regret[700]: 0.039653253312500025\n",
            "Running simulation: 2\n",
            "regret[0]: 0.3023839779999999\n",
            "regret[50]: 0.3023839779999999\n",
            "regret[100]: 0.14773736518749997\n",
            "regret[150]: 0.3503793677199999\n",
            "regret[200]: 0.03842052543625002\n",
            "regret[250]: 0.037267573081250005\n",
            "regret[300]: 0.03513761854999997\n",
            "regret[350]: 0.1059874452125\n",
            "regret[400]: 0.10435752356249994\n",
            "regret[450]: 0.10435752356249994\n",
            "regret[500]: 0.031235826999999994\n",
            "regret[550]: 0.031235826999999994\n",
            "regret[600]: 0.031235826999999994\n",
            "regret[650]: 0.039653253312500025\n",
            "regret[700]: 0.039653253312500025\n",
            "Running simulation: 3\n",
            "regret[0]: 0.3023839779999999\n",
            "regret[50]: 0.3023839779999999\n",
            "regret[100]: 0.3237519779999999\n",
            "regret[150]: 0.30887693512499986\n",
            "regret[200]: 0.03827922372625003\n",
            "regret[250]: 0.038498979553750035\n",
            "regret[300]: 0.03827922372625003\n",
            "regret[350]: 0.03513761854999997\n",
            "regret[400]: 0.03513761854999997\n",
            "regret[450]: 0.03513761854999997\n",
            "regret[500]: 0.033321987750000004\n",
            "regret[550]: 0.03363150300000001\n",
            "regret[600]: 0.031235826999999994\n",
            "regret[650]: 0.031235826999999994\n",
            "regret[700]: 0.030874186999999997\n",
            "Running simulation: 4\n",
            "regret[0]: 0.3023839779999999\n",
            "regret[50]: 0.3023839779999999\n",
            "regret[100]: 0.33388232759999986\n",
            "regret[150]: 0.10967521338249997\n",
            "regret[200]: 0.00817221116312511\n",
            "regret[250]: 0.03827922372625003\n",
            "regret[300]: 0.03827922372625003\n",
            "regret[350]: 0.03495190939999998\n",
            "regret[400]: 0.03495190939999998\n",
            "regret[450]: 0.03363150300000001\n",
            "regret[500]: 0.03363150300000001\n",
            "regret[550]: 0.031235826999999994\n",
            "regret[600]: 0.031235826999999994\n",
            "regret[650]: 0.031235826999999994\n",
            "regret[700]: 0.031235826999999994\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M49yfZ_S7yLE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "e5ec97cf-c241-4e11-8525-0deaab9b953f"
      },
      "source": [
        "x = regrets.cumsum(axis=-1)\n",
        "mean_regret = x.mean(axis=0)\n",
        "std_regret = x.std(axis=0)\n",
        "plt.plot(mean_regret)\n",
        "plt.fill_between(np.arange(nb_episodes), mean_regret - std_regret, mean_regret + std_regret, alpha=0.1)\n",
        "plt.ylabel('regret')\n",
        "\n",
        "# SAVE PSRL REGRET\n",
        "ucrl_regret = mean_regret"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD4CAYAAADrRI2NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3de3xc5X3n8c9vNJKsu2xLtmXL8h1f\nAGOMuCcETAgh6Ya0ZSnksiRl6zabNKS5km673e6rr93QNmlJNrsJTZrS3TQlSZtC0iwBHFMCAYPB\n96vku2TZknW1fJE0M7/945wRirGxbHRmRprv+/Wal86cmdH5BcVfPXqe5zyPuTsiIpI/YtkuQERE\nMkvBLyKSZxT8IiJ5RsEvIpJnFPwiInkmnu0CRqOmpsbnzp2b7TJERMaVV1999Zi71555flwE/9y5\nc1m/fn22yxARGVfM7MDZzqurR0Qkzyj4RUTyTKTBb2Z/YGbbzGyrmX3PzCaZ2TwzW2dmzWb2mJkV\nRVmDiIj8qsiC38xmAZ8EGt39MqAAuAd4CPgrd18IdAP3R1WDiIi8UdRdPXGgxMziQCnQBqwCfhi+\n/ijw/ohrEBGRESILfndvBf4SOEgQ+L3Aq0CPuyfCt7UAs872eTNbbWbrzWx9R0dHVGWKiOSdKLt6\nJgN3AvOAmUAZ8O7Rft7dH3H3RndvrK19wzRUERG5SFF29bwT2OfuHe4+BPwzcCNQHXb9ANQDrRHW\nICIiZ4gy+A8C15lZqZkZcCuwHVgL3BW+5z7g8QhrEBEZV5Ip5/RQkr7TQ5weSkZyjcju3HX3dWb2\nQ+A1IAFsAB4B/hX4RzP7s/Dct6OqQUQkl7k7g8kUQ0knkUwxmEwxcm+s+CSL5LqRLtng7n8C/MkZ\np/cC10R5XRGRXDWYCAJ+MJEikUyRjT0Qx8VaPSIi41Uy5UHYJ1IMJJPkwm63Cn4RkTHk7gyMaNUn\nUzmQ9GdQ8IuIvAWpVNBPP5hMMZRIkcjBoD+Tgl9E5AKkW/RD6X76cRD0Z1Lwi4icx8gB2aFkKtJr\nDSSS7DpynK2tfew80seX715BefHYRrWCX0RkhJFTLIfCoI+yTX+07zSbDvWwpbWXra197D56fPiv\niFnVJRzuOcUl0yvG9JoKfhHJe0PJVDAgG/EUy5Q7ze39bDrUw6aWXja39HC0bwCASYUxltVV8oFr\nG7hsVhWXzaxkbk0ZpUVjH9MKfhHJO0Mjum3OvGlqLCVTzu6jx9lwsIfXDnaz8VAPx08Ha1TWVhRz\nRX0VH7y2muX1VSyaXk48lpm9sRT8IjLhDSVfH4yNMuhPDyXZ0dbHxkM9bDzUw+aWXk4OBssuzJ5S\nwqol01jZMJkVs6uZUTUpmiJGQcEvIhNOMuUMJJKRB33fqSE2t/Sy8VAPm1p62NHWx1AyuNiC2jLe\nfekMVjRUs7JhMrUVxdEUcREU/CIyIaT76QeGkpFNsew4PjDcmt94sIfmjn4A4jFjaV0l91zdwBWz\nq1heX01VSWEkNYwFBb+IjEvpG6fSg7KpMW7WuzuHuk+xuaWHDQeDsG/pPgVASWEBl9dXsXrpfFbM\nrubSmZVMKiwY0+tHScEvIjnP3UmkPOyrD1ayHOtW/WAixa4jx9nUEvTNb27pofvkEACVJXFWzK7m\nN1bO4srZk7lkRuYGYqOg4BeRnJOJZRCOnx5iS2svmw4FffTbD/cxGN6cVT+5hOsXTGV5fTXLZ1Ux\nr7aMmEWzRPJIMTMKC4zCglj4GIfLMouIjEb6pqn0KpZRBH36RqlN4WDsnvZ+HCiIGUtmVHDXVfUs\nr69ieX0VU8ujH4g1oLAgRnxE0BfEov/lAgp+EcmCRNhlM5gMbphKpnzMb5o63HOKl/d1seFQD5sO\n9dDWexoI++dnVXHL2+eF/fNVlBRF3z9fELPhVnw66LMlsuA3s8XAYyNOzQf+C/D34fm5wH7gbnfv\njqoOEcm+xHC3TRD2Yz0QC3BiIMH6/d28uLeTV/Z3DQ/ETikr4or6Kn7r6tmsmF2dkRulzKCoIEY8\nHfSxGLEMteZHI8qtF3cBKwDMrIBgU/UfAQ8Ca9z9S2b2YPj8C1HVISKZlR54HUqmSCQ9srVuEqlg\nMHb9/m5e2d/FhoM9JFJOaVEBV82ZzG81zubqeVOYO7UUi7h/Ph4zCuOxIOxjRjyLrfnRyFRXz63A\nHnc/YGZ3AjeH5x8FnkXBLzIuDYd7Kvga5To33ScG2Xa4j62tvWw93Mu2w33Dd8UuqC3jnmtmc8OC\nGq6or4o0eGNmYWv+9a6bqH+xjLVMBf89wPfC4+nu3hYeHwGmn+0DZrYaWA3Q0NAQeYEicm7p6ZSZ\nCnmAtt6gj/7VA91sbe2jtSfouikwY+G0cu64bAZXNkxmZUN1ZIOxBsTDkC/K8ABslMwj3gDSzIqA\nw8Cl7n7UzHrcvXrE693uPvnNvkdjY6OvX78+0jpFJODuwVz51Otz5qMYfD1T/0CCVw908/K+Ll7e\n18XBrpNA0Ee/vL5qeMXKJTMqIxuMzaUB2LFgZq+6e+OZ5zPR4r8DeM3dj4bPj5pZnbu3mVkd0J6B\nGkTkLM4W8pnaUap/IMGGg928djCYdbOz7ThJdyYVxriyYTK/sXIW18ydwvzaski6UsygMBajMJ6b\nA7BRykTw38vr3TwATwD3AV8Kvz6egRpEhNdXqcx0yEMw6LvtcF/Qot/fxbbWPpLuFBXEWDazkv9w\n/RyunjeFy2dVURQf+5Z2egC2MBYEfa4PwEYp0uA3szLgNuB3R5z+EvB9M7sfOADcHWUNIvkq0ztJ\nnann5CDb2/rYfriPbYeDpYpPDiaJGSytq+TD18/h6rmTuby+iuL42HbdpKdTjrwDdrwNwEYp0uB3\n9xPA1DPOdRLM8hGRMZQMp1CmNwJPZrA1f2owya6jx9l+uG847NODsQbMqynjjstmcPXcKVw1ZzKV\nY7xyZfou2KJ4jOJ4LK9b86OhO3dFxqlM3BR1NqeHkjQd7WdHWx87jxxnR1sf+ztPkP49M6NyEstm\nVvLrK2dxaV0li2dUUDbGm4WPDHq16C+cgl9kHEh326RviBpKekaC/vRQkqb2fna29bHjyHF2tR1n\n37ETJMNrTy4tZGldJTcvrmVpXSWXzqyMZGplQczCgdhgtk08pqB/KxT8IjkofWNUei2bTAzCDiSS\nNLf3s6PtODuP9LGj7Tj7On415JfUVfL2S2pYOqOSJXUVTKsojiSAC2JGUXgnbFFB/sy2yRQFv0iW\npYM93aKP+sYoCNaeb27vHw74nUf62NNxYnhcoLqkkCV1Fbx9YQ1L6ipYWlcZWcinFRYE/fPqo4+e\ngl8kw4YHYBMphlLR7Qc7Umv3KV7e3xX0y7cdZ09H//BfEVUlhSyZUcGHr6thyYwKltRVMKNyUqQh\nn+6jL4wH3TZq1WeWgl8kA9Ibi5waTA5v9hGlgUSSDQd7eHFPJy/u6eRAeBdsZUmcJTMq+cC1DSyt\nq2TJjArqqqIN+TQDiuIxJhUWKOizTMEvEpFkyjk5mIhsY5EzHe45xS/DoF9/oIvTQymKCmJcNWcy\nv3lVPdfPn8rsKSUZHRRNL4GQ7sLRgGxuUPCLjJGRm4tEsfn3mVLubDzYw9pd7azb2zXcqq+fXMK/\nWz6T6xdM5ao5kzO2CXh65k28wIKVK/NoCYTxRsEv8hYkkilODSUZSGTmhqnuE4NsPNTDhkM9rN3Z\nTvvxAYrjMVbOCda2uWFhDQ1TSiOvY2TIax79+KPgF7lAA4nk8OBs1F04iWSKLa29vLg36MLZfbQf\ngOJ40IXziVULuWlRbaRbB45csTIeU8hPBAp+kVFIppzBRIqTg4nIw/5I72le2ts5vIXgiYEkBWYs\nr6/iY+9YwMo51Sytq4xkyeCRN0op5CcuBb/IWaSXK06vZjmQiGYmTjLlNLf3s7mlh80tvWxu6eVI\nX7Ap+PTKYt65dDo3LJhK45wplE8a23+uCvn8peAX4fUdpgYTwcBslCtZHuw8yS+aO3hpbxdbW3uH\ntw+sLS9meX0V914zm2vmTWFezditQ6+1bWQkBb/kpfS8+pFr00cV9Ilkio2HenihuZNfNHdwqCtY\ntXJ+uGLlFbOruXxW1ZjPp9e8eTkXBb/klUQyxYnBJANDyUiXReg9OcQv9x7j+aZjvLS3i/6BBIUF\nxlVzJvNbjbO5cWENM6tLxvy6BpQUFTCpsEALmck5KfhlwnN3BhIpTg4mGYrorll3Z9+xE0GrvqmD\nLa29pDzYL/aWJbW8bWEN18ybQmlRNP/k4jELAj9eoJa9nFfUO3BVA98CLgMc+G1gF/AYMBfYD9zt\n7t1R1iH5KZlyTg0lOTmYiGQ9nKFkig0He/hFUwcvNHcObzyyeHoFH7lhLm9bVMPSukpiEbW6Y2aU\nhq37AoW9XICoW/wPA0+6+11mVgSUAn8IrHH3L5nZg8CDwBcirkPywPB+shFuTDKYSPHink6e2n6E\nX+7p5ORgkuJ4jMa5k/nQdQ3cuLCG6ZWTxvy6aTELFjQL+u61BIJcnMiC38yqgJuAjwC4+yAwaGZ3\nAjeHb3sUeBYFv1ygN+wnG+Eql0PJFOv3d7N2Vztrd7bTdzrB5NJCbls2nbcvquHquVMiXRbBgOLC\nAkoKCyLZhFzyT5Qt/nlAB/AdM7sCeBV4AJju7m3he44A08/2YTNbDawGaGhoiLBMGQ/OnIUTVV99\n2kAiybq9Xfx8ZzvPNx/j+OkEpUUF3LSoltsvm841c6dEumZ8evrlpMICtexlzEUZ/HFgJfD77r7O\nzB4m6NYZ5u5uZmdtp7n7I8AjAI2NjZnbNVpyQnrj8PSCZ5lYB2cwkWLdvk6e3n6UXzQd4+RgkspJ\ncW5aVMuqJdO4et5kiuPRtuzT0y+1kqVEKcrgbwFa3H1d+PyHBMF/1Mzq3L3NzOqA9ghrkHEgfZds\nIpX5jcNPDCR4cU8na3e1D/fZV5bEuW3ZdFYtmUbjnMmRtezjMaMwHgvuni0w7TolGRNZ8Lv7ETM7\nZGaL3X0XcCuwPXzcB3wp/Pp4VDVIbhq5fHGm9pMdqefkIM81HePZXe28sq+bwWRquM/+lsXTuHru\n2Ie9GUwqLHh92WLNsZcsinpWz+8D3w1n9OwFPgrEgO+b2f3AAeDuiGuQLBrZN58I++az0W93tO80\nz+7q4Nld7Ww81EPKoa5qEr951SzecUkty+urx3xKpAHF8QKKC2MZWxNfZDQiDX533wg0nuWlW6O8\nrmRPNvrmz6Z/IMGWll42Huph3b5OdrQdB4JlEj5yw1xuXjyNS6aXj3mrO91PXxzXoKzkLt25K2/J\ncLdNIpXRvvkzdfYPsPFQDxsP9bDpUC9N7cdJebAC5bK6Sj5xy0Lesbg2kk1KzHh9br3unJVxQMEv\no5JKBatXptxf77aJcO78m3F3DvecHg76DYe6hxc+K47HuHxWFb994zxWzK7msllVkWxSEo9ZsB5O\ngUU600ckCgp+eYP0EsUjV67M9ADsSCl39nT0s/Hg6y36jv4BAConxblidjV3rpjFitnVLJlREckG\nJfB62GuJBBnvFPwCBF02p8PtBLM1AJvWP5Bg++E+trT2sqW1l22tvfSdTgBQW1HMlQ3VrJgdPObV\nlkW2Fg4EXUWlRQUUxxX2MnEo+AV3p/vkUFb655Mp52DXSbYf7mNzSw9bW/vY09GPEwyUzqsp45Yl\n04aDfqzXrD8bLZEgE52CX+g9lbnQT6acra29PN98jM0tvew+enx4B6ry4jiXzqzkliXzuGxWFZfN\nrBrz7QbfTMyMsuIg8DUbRyYyBX8eG0ykMhL6vaeG2HCwm180HeOF5mN0nxwiHjOW1lXy3svrWFJX\nwbK6SubWRNttczaaay/5SMGfp1Ipjyz0e08NsfFgD68e7Oa1A900twddN+XFcW5YMJWbLqnl+vlT\nM9qaP1NhQYwSLYAmeUrBn6dODiXHLPR7Tw6x8dAbg744HmN5fRWrb5rPlQ3BvrLZWI/GgHhBjIKY\nURhOv9RAreQzBX8ecndODiYu6rMpd1q6TrH1cC+bW3rZdKiHvcdOAL8a9CvnTGZZXWVWBkeDgI9R\nVBCsixPV9E6R8UrBn4dODSVHfeNV76khtqanVR7uY0dbH8fDqZVlxQUsn1XNbcumZy3ozQhWt4wH\nK1wWxmK6c1bkPBT8eSg9i+ZMKXf2HzvB1tY+Nrf2sKWll/2dJwEoMGPBtDJuXTKNS2dWsWxmJfNq\nyjLeZTKyNa+ljEUujoI/z5waTL5h4bTjp4f4y6d280K40xRAZUmc5bOquePyOpbPqmJpXWUkSx+8\nGbXmRaKh4M8zJ0b07fefTvDCnmP8r7V76Ogf4L2X13HF7CqWz6pm9pSSjM92iceCFrxa8yLRUvDn\nkdNDr7f2t7T08ukfbKTvVIL5NWV880NXcXl9VcZqSXfZFBYY8XAHKk2rFMkMBX8eSfftt3af4lOP\nbaS6tJD/8euXs6Khmngs+tZ1eimE0qICzbQRyaJIg9/M9gPHgSSQcPdGM5sCPAbMBfYDd7t7d5R1\nCAwkkuFyyin++PGtmMHX7r2SmdUlkV87ZkZJUQGlhVqrXiQXZKLZdYu7r3D39E5cDwJr3H0RsCZ8\nLhE6PZSk71TQt//N5/ay7XAfX7xjSWShHzNjUryAykmFTC0roraimPLiuEJfJEdko6vnTuDm8PhR\n4FngC1moIy8kU07fqSEcWLevk79/8QDvXzGTW5dOH7NrFMSMonh6UDamu2JFclzUwe/AU2bmwDfd\n/RFguru3ha8fAcYugeQNesPQ7zoxyH99Yjvza8r4g9sueUvfMx4GfXo+vVryIuNL1MH/NndvNbNp\nwNNmtnPki+7u4S+FNzCz1cBqgIaGhojLnJhODCQYSgYbnv+3H2/nxECC/3nvlRe8CmU8ZhSGLXoF\nvcj4F2nwu3tr+LXdzH4EXAMcNbM6d28zszqg/RyffQR4BKCxsTGbG0KNS+7OicEEiWSKh57cxYt7\nO/nCuxezYFr5m34uvXF44fBD0yxFJprIBnfNrMzMKtLHwLuArcATwH3h2+4DHo+qhnx2cjBYj+cf\nXj7IE5sO8+Hr5vAbK+t/5T1G0JovjscoL44ztayIaRWTqC4toqw4TlFcSxaLTERRtvinAz8KgyMO\n/IO7P2lmrwDfN7P7gQPA3RHWkJdSqaC139k/wN/9cj9vX1TDJ1YtHH7dgJKiAsqL4wp2kTwUWfC7\n+17girOc7wRujeq6Av2DCdzhq2uaGUyk+OSqRcDrgV9aFNfMG5E8pjt3J5hEMsWpwSTN7f08ue0I\n990wh4appcTMqCop1ObhIqLgn2j6B4Ibtf7ul/spLSrgg9fOwQwmlxZq0TMRATJz565kyGAixUAi\nxYHOEzyz/Sh3XVVPVUkhVSUKfRF5ndJgAukfSJBy58tP7aa4MMa91zRQWBCjOJ7ZdfRFJLcp+CeI\n00PBImz/58UDrNvXxR+88xKmlBVRmuHNU0Qk9yn4JwB3p38gwaZDPXzz3/Zy27Lp3LliJkUFsQu+\nS1dEJj4F/wRwYjBJV/8gf/z4VuqqJ/HgHUswM0qLFfoi8kaa1TPOJVNO5/EB/uhfttLZP8i37muk\nvDge3pGr4BeRNxpVi9/MHhjNOcm8/oEEf/Ljbaw/0MUfvmcpS+sqASgr1u90ETm70Xb13HeWcx8Z\nwzrkIiRTzpNb2nh2Vwcfu3kB711eB0BxXH37InJub9osNLN7gQ8A88zsiREvVQBdURYm59d7cpCv\nPLObS6aX84Frg6WrJ8ULqCotzHJlIpLLztcf8EugDagBvjzi/HFgc1RFyfmlUs4PX2vhaN8AX7xj\nKaWFwWqa6uIRkfN505Rw9wMEK2heb2ZzgEXu/oyZlQAlBL8AJAv6Tg/x9y8eYMmMClYtqaWypCjb\nJYnIODHawd3fAX4IfDM8VQ/8S1RFyZtzd57YdJiW7lP89o3zqJikrh0RGb3RDu5+HLgR6ANw9yZg\nWlRFyZvrP53gOy/sZ35NGb+2vE5r6ovIBRlt8A+4+2D6iZnFCTZSlyz46dYj7Dt2go/eOI9S9emL\nyAUabfD/m5n9IVBiZrcBPwB+HF1Zci79pxP8zXN7qZ9cwvuuqMt2OSIyDo02+B8EOoAtwO8CPwX+\naDQfNLMCM9tgZj8Jn88zs3Vm1mxmj5mZRiVHKZVyvr/+IM0d/Xzs5gVUlqhvX0Qu3HmD38wKgP/j\n7n/j7v/e3e8Kj0fb1fMAsGPE84eAv3L3hUA3cP8FV52nmtqP8/W1e1hWV8m/U9++iFyk8wa/uyeB\nORfTMjezeuC9wLfC5wasIpghBPAo8P4L/b75qO/UEJ/9wWZODib54/cupbxYrX0RuTijHRncC7wQ\n3r17In3S3b9yns/9NfB5gjt9AaYCPe6eCJ+3ALPO9kEzWw2sBmhoaBhlmRPXw2ua2NLay3//9cu4\nfHY1MW2WLiIXabR9/HuAn4TvrxjxOCcz+zWg3d1fvZjC3P0Rd29098ba2tqL+RYTxv5jJ/i/Lx3g\nXcum8/4rZ1GumTwi8haMKkHc/U8v4nvfCLzPzN4DTAIqgYeBajOLh63+eqD1Ir53XvnLp3aRcudz\nty+mtEihLyJvzahSxMx+zBvn7fcC64FvuvvpMz/j7l8Evhh+/mbgs+7+QTP7AXAX8I8Eq34+ftHV\n54ENB7v5181tfOj6OSya/qZ/ZImIjMpou3r2Av3A34SPPoJ1ei4Jn1+ILwCfNrNmgj7/b1/g5/PK\nQ0/upKIkzgOrFmW7FBGZIEbbb3CDu1894vmPzewVd7/azLad78Pu/izwbHi8F7jmQgvNRz/feZSX\n9nbx2XctpqaiONvliMgEMdoWf7mZDU+tCY/Lw6eDZ/+IvBXuzlfXNDOtopiP3jgn2+WIyAQy2hb/\nZ4DnzWwPYMA84D+ZWRnBXHwZY881dbDxUA//+T1LKdOcfREZQ6Od1fNTM1sELAlP7RoxoPvXkVSW\n5762ppma8iI+fL1a+yIytka7Hn8p8DngE+6+CZgdztOXCLzQdIz1B7pZfdN87Z0rImNutH383yHo\ny78+fN4K/FkkFQlf/XkTU8uK+PB1c7NdiohMQKMN/gXu/ufAEIC7nyTo65cx9tLeTtbt6+L+t8+j\npEitfREZe6MN/sFwn10HMLMFwEBkVeWxr61porq0kI/cMDfbpYjIBDWaZZkN+AbwJEHf/neBNQSL\nr8kYWr+/ixf2dPLbN87T0gwiEpnzpou7u5l9DrgZuI6gi+cBdz8WcW1556trmqgqKeSjN87Ndiki\nMoGNtln5GjDf3f81ymLy2caD3TzXdIxP3rqQikmaty8i0Rlt8F8LfNDMDhCsx28Efwwsj6yyPPOV\nZ3ZTMSnO/W+bl+1SRGSCG23w3x5pFXlu3d5Ontt9jE/duoiqEm1BLCLRGu2duweiLiRfuTtfenIn\nU8uK+J2b1NoXkeiNdjqnRGTNjqNsONjDJ1Yt1Jo8IpIRCv4sSqach57cxewpJXzoOq3JIyKZoeDP\nosc3ttLU3s/n3rWYwgL9KEQkMyJLGzObZGYvm9kmM9tmZn8anp9nZuvMrNnMHjOzvBzNHEgk+cpT\nu1lWV8mvLZ+Z7XJEJI9E2cwcAFa5+xXACuDdZnYd8BDwV+6+EOgG7o+whpz1/fUttPSc4gt3LCEW\n07JHIpI5kQW/B/rDp4Xhw4FVwA/D848C74+qhlw1mEjxv9Y2c1XDZG5aVJPtckQkz0TasWxmBWa2\nEWgHngb2AD3ungjf0gLMOsdnV5vZejNb39HREWWZGfdPr7XQ1nuaT75zEcFSSCIimRNp8Lt70t1X\nAPUEG6wvOc9HRn72EXdvdPfG2trayGrMtKFkiq+vbeaK+iq19kUkKzIylcTde4C1BBu5VJtZ+sax\neoJNXfLGj15rpaX7FA+otS8iWRLlrJ5aM6sOj0uA24AdBL8A7grfdh/weFQ15JpEMsX/XNvMZTMr\nuWXxtGyXIyJ5KsoWfx2w1sw2A68AT7v7T4AvAJ82s2ZgKvDtCGvIKY9vPMzBrpN88la19kUkeyLb\n7cPdNwNXnuX8XoL+/rySTDlf+3kTS+squG3Z9GyXIyJ5TLeLZshPNh9mf+dJPrlKrX0RyS4FfwYE\nrf1mLplezu2Xzsh2OSKS5xT8GfDTLW00t/fz+6sW6S5dEck6BX/EUmHf/sJp5bzn8rpslyMiouCP\n2s+2HWH30X5+f9VCCtTaF5EcoOCPUCrlPLymiXk1ZVqBU0RyhoI/Qk/vOMrOI8fV2heRnKLgj4i7\n89U1TcyZWsr7rlBrX0Ryh4I/Imt2tLPtcB+fuGUhce2uJSI5RIkUgXRrf/bkEt5/5VlXnRYRyRoF\nfwSe3dXB5tZePrFqofbSFZGco1QaY+7BTJ5Z1SX8+pX12S5HROQNFPxj7LmmY2w81MPHb1lIUVz/\neUUk9yiZxpC78/Azu5lZNYm7rlJrX0Ryk4J/DL3Q3MlrB3v4mFr7IpLDlE5jJOjb382Myknc3ajW\nvojkrii3XpxtZmvNbLuZbTOzB8LzU8zsaTNrCr9OjqqGTHppbxev7O/m994xn+J4QbbLERE5pyhb\n/AngM+6+DLgO+LiZLQMeBNa4+yJgTfh83Ht4zW6mVRRzzzUN2S5FRORNRRb87t7m7q+Fx8cJNlqf\nBdwJPBq+7VHg/VHVkCnr9nby0t4ufvcdC5hUqNa+iOS2jPTxm9lcgv131wHT3b0tfOkIcNYNaM1s\ntZmtN7P1HR0dmSjzoj28poma8iI+oNa+iIwDkQe/mZUD/wR8yt37Rr7m7g742T7n7o+4e6O7N9bW\n1kZd5kV7eV8Xv9zTye+9YwElRWrti0juizT4zayQIPS/6+7/HJ4+amZ14et1QHuUNUTt4TW7qSkv\n4oPXzsl2KSIioxLlrB4Dvg3scPevjHjpCeC+8Pg+4PGoaojaK/u7eKFZrX0RGV/iEX7vG4EPA1vM\nbGN47g+BLwHfN7P7gQPA3RHWEKmHn2lSa19Exp3Igt/dnwfOte3UrVFdN1PW7e3k+eZj/NF7l6q1\nLyLjiu7cvQjuzkNP7mRG5SQ+dJ1a+yIyvij4L8KaHe28drCHB965SPP2RWTcUfBfoGTK+Yuf7WJ+\nTRn/Xitwisg4pOC/QI9vbM+KqBUAAAmISURBVGXX0eN85l2LtZeuiIxLSq4LMJBI8pWnd3P5rCru\nuGxGtssREbkoCv4L8L11B2npPsXn372YWOxcE5ZERHKbgn+U+gcSfO3nzVw/fypvW1iT7XJERC6a\ngn+U/vb5fXSeGOTz715McFOyiMj4pOAfha4Tg/zNc3u5/dLpXNkwIfaNEZE8puAfhf/9bDMnBhN8\n9l2Ls12KiMhbpuA/j8M9p3j0xQP85sp6Fk2vyHY5IiJvmYL/PB5+pgkcPnXbJdkuRURkTCj430Rz\nez8/ePUQH7puDrOqS7JdjojImFDwv4kvP7WLksICPn7LgmyXIiIyZhT857DpUA//b+sRfuem+Uwt\nL852OSIiY0bBfw5/8bNdTCkr4j++fX62SxERGVNRbr34t2bWbmZbR5ybYmZPm1lT+DUnJ8U/33SM\n55uP8fFbFlJeHOUmZSIimRdli//vgHefce5BYI27LwLWhM9zirvz5z/byazqEj54bUO2yxERGXOR\nBb+7Pwd0nXH6TuDR8PhR4P1RXf9iPbn1CJtbevmUNlkRkQkq03380929LTw+Akw/1xvNbLWZrTez\n9R0dHRkpLpFM8RdP7WLRtHJ+Y6U2WRGRiSlrg7vu7oC/yeuPuHujuzfW1tZmpKZ/eq2FvR0n+Ozt\niynQsssiMkFlOviPmlkdQPi1PcPXP6fTQ0n++pkmVsyu5l3LzvmHiIjIuJfp4H8CuC88vg94PMPX\nP6f/+9IB2npPa9llEZnwopzO+T3gRWCxmbWY2f3Al4DbzKwJeGf4POv6Tg/x9bXNvH1RDTcs0CYr\nIjKxRTZJ3d3vPcdLt0Z1zYv1ref20n1yiM/fviTbpYiIRC7v79ztOD7At57fx3uX13F5fVW2yxER\niVzeB//X1zYzkEjxGS27LCJ5Iq+D/1DXSb677gB3N9Yzv7Y82+WIiGREXgf/Xz2zm5gZn7x1UbZL\nERHJmLwN/l1HjvOjDa185Ia51FVpkxURyR95G/x/8bNdlBfH+djN2mRFRPJLXgb/qwe6eGbHUX7v\nHQuoLi3KdjkiIhmVd8Hv7jz05C5qyov56I1zs12OiEjG5V3w/9vuDl7e18Unb11IaZE2WRGR/JNX\nwZ9KOX/+5C5mTynhnqu1yYqI5Ke8Cv6fbGlje1sfn7ltMUXxvPqfLiIyLG/SbyiZ4stP7WLJjAre\nd8XMbJcjIpI1eRP8j71yiAOdJ/nc7YuJaZMVEcljeRH8pwaTPLymicY5k1m1ZFq2yxERyaq8CP7v\n/HIfHccH+MIdS7TJiojkvQkf/L0nh/jGs3tYtWQaV8+dku1yRESybsIH/zee28PxgQSfu31xtksR\nEckJWQl+M3u3me0ys2YzezCq6xztO813XtjH+66YydK6yqguIyIyrmQ8+M2sAPg6cAewDLjXzJZF\nca2vrmkikXQ+rU1WRESGZaPFfw3Q7O573X0Q+Efgzigu1DCllN+5aT5zppZF8e1FRMalbCxWMws4\nNOJ5C3DtmW8ys9XAaoCGhotbXuF336Ell0VEzpSzg7vu/oi7N7p7Y21tbbbLERGZMLIR/K3A7BHP\n68NzIiKSAdkI/leARWY2z8yKgHuAJ7JQh4hIXsp4H7+7J8zsE8DPgALgb919W6brEBHJV1nZicTd\nfwr8NBvXFhHJdzk7uCsiItFQ8IuI5BkFv4hInjF3z3YN52VmHcCBi/x4DXBsDMuJQq7XmOv1gWoc\nC7leH+R+jblW3xx3f8ONUOMi+N8KM1vv7o3ZruPN5HqNuV4fqMaxkOv1Qe7XmOv1pamrR0Qkzyj4\nRUTyTD4E/yPZLmAUcr3GXK8PVONYyPX6IPdrzPX6gDzo4xcRkV+VDy1+EREZQcEvIpJnJnTwZ2pv\n3/PU8Ldm1m5mW0ecm2JmT5tZU/h1cnjezOyrYb2bzWxlhmqcbWZrzWy7mW0zswdyqU4zm2RmL5vZ\nprC+Pw3PzzOzdWEdj4WrvWJmxeHz5vD1uVHWd0atBWa2wcx+kos1mtl+M9tiZhvNbH14Lid+zuE1\nq83sh2a208x2mNn1OVbf4vC/XfrRZ2afyqUaR8XdJ+SDYOXPPcB8oAjYBCzLQh03ASuBrSPO/Tnw\nYHj8IPBQePwe4P8BBlwHrMtQjXXAyvC4AthNsB9yTtQZXqc8PC4E1oXX/T5wT3j+G8DHwuP/BHwj\nPL4HeCyDP+9PA/8A/CR8nlM1AvuBmjPO5cTPObzmo8B/DI+LgOpcqu+MWguAI8CcXK3xnLVnu4AI\nfyjXAz8b8fyLwBezVMvcM4J/F1AXHtcBu8LjbwL3nu19Ga73ceC2XKwTKAVeI9iu8xgQP/PnTbDk\n9/XhcTx8n2WgtnpgDbAK+En4jz3Xajxb8OfEzxmoAvad+d8hV+o7S73vAl7I5RrP9ZjIXT1n29t3\nVpZqOdN0d28Lj48A08PjrNccdjlcSdCqzpk6wy6UjUA78DTBX3M97p44Sw3D9YWv9wJTo6wv9NfA\n54FU+HxqDtbowFNm9qoF+1pD7vyc5wEdwHfC7rJvmVlZDtV3pnuA74XHuVrjWU3k4B8XPGgG5MSc\nWjMrB/4J+JS79418Ldt1unvS3VcQtKqvAZZkq5azMbNfA9rd/dVs13Ieb3P3lcAdwMfN7KaRL2b5\n5xwn6Bb93+5+JXCCoNtkWLb/f5gWjtW8D/jBma/lSo1vZiIHfy7v7XvUzOoAwq/t4fms1WxmhQSh\n/113/+dcrdPde4C1BN0m1WaW3kxoZA3D9YWvVwGdEZd2I/A+M9sP/CNBd8/DOVYj7t4afm0HfkTw\nSzRXfs4tQIu7rwuf/5DgF0Gu1DfSHcBr7n40fJ6LNZ7TRA7+XN7b9wngvvD4PoI+9fT5/xDOBLgO\n6B3x52NkzMyAbwM73P0ruVanmdWaWXV4XEIw/rCD4BfAXeeoL133XcDPw1ZYZNz9i+5e7+5zCf6/\n9nN3/2Au1WhmZWZWkT4m6KPeSo78nN39CHDIzBaHp24FtudKfWe4l9e7edK15FqN55btQYYoHwQj\n6rsJ+oP/c5Zq+B7QBgwRtGjuJ+jLXQM0Ac8AU8L3GvD1sN4tQGOGanwbwZ+mm4GN4eM9uVInsBzY\nENa3Ffgv4fn5wMtAM8Gf3MXh+Unh8+bw9fkZ/pnfzOuzenKmxrCWTeFjW/rfRK78nMNrrgDWhz/r\nfwEm51J94XXLCP46qxpxLqdqPN9DSzaIiOSZidzVIyIiZ6HgFxHJMwp+EZE8o+AXEckzCn4RkTyj\n4BcRyTMKfhGRPPP/AQHscH3z1e2TAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myPuG9Er7yLL",
        "colab_type": "text"
      },
      "source": [
        "# Posterior Sampling for RL\n",
        "\n",
        "At each iteration, PSRL samples one MDP from the posterior distribution and run the associated optimal policy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVI74hM2g82r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gxP6B9r7yLN",
        "colab_type": "text"
      },
      "source": [
        "Implement posterior sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cl9YwtfJ7yLP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def PSRL(mdp, H, nb_episodes, VSTAR=0):\n",
        "    reward_prior = [1,1]\n",
        "    S, A = mdp.Ns, mdp.Na\n",
        "    Phat = np.ones((S,A,S)) / S\n",
        "    Rhat = np.zeros((S,A))\n",
        "    N_sas = np.zeros((S,A,S), dtype=np.int)\n",
        "    N_sa = np.zeros((S,A), dtype=np.int)\n",
        "    regret = np.zeros((nb_episodes,))\n",
        "    \n",
        "    for k in range(nb_episodes):\n",
        "        \n",
        "        # compute policy\n",
        "        # 1. sample MDP\n",
        "        R = np.zeros_like(Rhat)\n",
        "        P = np.zeros((S, A, S))\n",
        "        for s in range(S):\n",
        "            for a in range(A):\n",
        "                # sample transition matrix\n",
        "                # P[s, a] follows a dirichlet Dirichlet distribution of parameters N_sas[s, a,:] + 1\n",
        "                P[s, a] = ...\n",
        "\n",
        "                # posterior for Bernoulli rewards\n",
        "                N = N_sa[s, a]\n",
        "                v = N * Rhat[s, a]\n",
        "                a0 = reward_prior[0] + v\n",
        "                b0 = reward_prior[1] + N - v\n",
        "                p = np.random.beta(a=a0, b=b0, size=1).item()\n",
        "                R[s, a] = p\n",
        "        \n",
        "        # 2. compute optimal policy\n",
        "        V, policy = ...\n",
        "        \n",
        "        # execute policy\n",
        "        initial_state = state = mdp.reset()\n",
        "        for h in range(H):\n",
        "            action = policy[h][state]\n",
        "            next_state, reward, done, _ = mdp.step(action)\n",
        "            \n",
        "            # update estimates (Rhat, N_sa, N_sas)\n",
        "            ...\n",
        "            \n",
        "            state = next_state\n",
        "        \n",
        "        # update regret\n",
        "        Vpi = policy_evaluation(mdp.P, mdp.R, H, policy)\n",
        "        regret[k] = VSTAR[0][initial_state] - Vpi[0][initial_state]\n",
        "        \n",
        "        if k % 50 == 0:\n",
        "            print(\"regret[{}]: {}\".format(k, regret[k]))\n",
        "\n",
        "    return regret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zT38aVOB7yLU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb_repetitions = 5\n",
        "nb_episodes = 750\n",
        "regrets = np.zeros((nb_repetitions, nb_episodes))\n",
        "for it in range(nb_repetitions):\n",
        "    print(\"Running simulation: {}\".format(it))\n",
        "    regrets[it] = PSRL(mdp=env, H=H, nb_episodes=nb_episodes, VSTAR=Vstar)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQhP7cpn7yLY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = regrets.cumsum(axis=-1)\n",
        "mean_regret = x.mean(axis=0)\n",
        "std_regret = x.std(axis=0)\n",
        "plt.plot(mean_regret)\n",
        "plt.fill_between(np.arange(nb_episodes), mean_regret - std_regret, mean_regret + std_regret, alpha=0.1)\n",
        "plt.ylabel('regret')\n",
        "\n",
        "# SAVE PSRL REGRET\n",
        "psrl_regret = mean_regret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phHZWwdJ7yLc",
        "colab_type": "text"
      },
      "source": [
        "Compare algorithms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvcqwaJY7yLd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "plt.plot(ucrl_regret, label='UCRL-H')\n",
        "plt.plot(psrl_regret, label='PSRL')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "it2tQtxL7yLi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}